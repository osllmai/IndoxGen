{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:46:43.809277Z",
     "start_time": "2024-09-23T12:46:43.755539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional, List, Dict, Any, Callable\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "class SyntheticDataGenerator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            generator_llm: Callable,\n",
    "            judge_llm: Callable,\n",
    "            columns: List[str],\n",
    "            example_data: List[Dict[str, Any]],\n",
    "            real_data: Optional[List[Dict[str, Any]]] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the SyntheticDataGenerator with LLMs, columns, and example/real data.\"\"\"\n",
    "        self.generator_llm = generator_llm\n",
    "        self.judge_llm = judge_llm\n",
    "        self.columns = columns\n",
    "        self.example_data = example_data\n",
    "        self.real_data = real_data\n",
    "        self.generated_data = []\n",
    "        self.feedback_history = []\n",
    "\n",
    "    def generate_data(self, num_samples: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate a specified number of synthetic data samples with feedback and improvement.\"\"\"\n",
    "        for _ in range(num_samples):\n",
    "            while True:\n",
    "                generated = self._generate_single_data_point()\n",
    "                score = self._judge_data_point(generated)\n",
    "\n",
    "                if score >= 0.9:  # Perfect score\n",
    "                    self.generated_data.append(generated)\n",
    "                    break\n",
    "                elif score >= 0.5:  # Medium score\n",
    "                    if self._ask_human_feedback(generated):\n",
    "                        self.generated_data.append(generated)\n",
    "                        break\n",
    "                    else:\n",
    "                        self._inform_generator(generated, score, \"Human rejected\")\n",
    "                else:  # Low score\n",
    "                    self._inform_generator(generated, score, \"Low score\")\n",
    "\n",
    "        return self.generated_data\n",
    "\n",
    "    def _generate_single_data_point(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a single data point using the generator LLM.\"\"\"\n",
    "        system_prompt = \"You are a synthetic data generator. Generate one realistic data based on the given examples and criteria.\"\n",
    "        prompt = self._create_generation_prompt()\n",
    "    \n",
    "        # Call the generator LLM (Hugging Face in this case)\n",
    "        generated = self.generator_llm(prompt, system_prompt=system_prompt, temperature=0.7, top_p=0.85)\n",
    "        \n",
    "        # Print the raw output for debugging\n",
    "        # print(\"Raw output from LLM:\", generated)\n",
    "        \n",
    "        try:\n",
    "            # Attempt to parse the generated data as JSON\n",
    "            return json.loads(generated)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse generated data as JSON: {generated}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "    def _judge_data_point(self, data: Dict[str, Any]) -> float:\n",
    "        \"\"\"Judge the generated data using the judge LLM.\"\"\"\n",
    "        system_prompt = \"You are a data quality judge. Evaluate the given data based on the criteria and return a score between 0 and 1. It's important to only send score without any description\"\n",
    "        criteria = self._create_judge_criteria()\n",
    "        prompt = f\"Data to evaluate: {json.dumps(data)}\\n\\nCriteria:\\n{criteria}\\n\\nProvide only a numeric score between 0 and 1.\"\n",
    "        score_str = self.judge_llm(prompt, system_prompt=system_prompt)\n",
    "    \n",
    "        try:\n",
    "            score = float(score_str)\n",
    "            if score < 0 or score > 1:\n",
    "                raise ValueError(f\"Score out of bounds: {score}\")\n",
    "            return score\n",
    "        except ValueError:\n",
    "            print(f\"Failed to parse judge score: {score_str}. Defaulting to 0.0\")\n",
    "            return 0.0\n",
    "\n",
    "    def _ask_human_feedback(self, data: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Simulate asking for human feedback on the generated data.\"\"\"\n",
    "        print(\"\\nPlease review this generated data point:\")\n",
    "        for col, value in data.items():\n",
    "            print(f\"{col}: {value}\")\n",
    "        return input(\"Is this data acceptable? (y/n): \").lower() == 'y'\n",
    "\n",
    "    def _inform_generator(self, data: Dict[str, Any], score: float, reason: str):\n",
    "        \"\"\"Provide feedback to the generator based on the judged data.\"\"\"\n",
    "        feedback = f\"Generated data: {json.dumps(data)}\\nScore: {score}\\nReason: {reason}\"\n",
    "        self.feedback_history.append(feedback)\n",
    "        print(f\"Feedback for generator: {feedback}\")\n",
    "\n",
    "    def _create_generation_prompt(self) -> str:\n",
    "        \"\"\"Create the prompt for the generator LLM to produce a data point.\"\"\"\n",
    "        random_variation = random.choice([\n",
    "            \"Please ensure diversity in the generated data.\",\n",
    "            \"Generate a new data point with slight variation from the examples.\",\n",
    "            \"Ensure the generated data is fresh and distinct from previous examples.\"\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"Generate synthetic data with the following columns: {', '.join(self.columns)}\\n\"\n",
    "        prompt += \"The data should be similar to the following examples:\\n\\n\"\n",
    "        for example in self.example_data:\n",
    "            prompt += json.dumps(example) + \"\\n\"\n",
    "        if self.real_data:\n",
    "            prompt += \"\\nAdditional real data for reference:\\n\"\n",
    "            for real in self.real_data:\n",
    "                prompt += json.dumps(real) + \"\\n\"\n",
    "        if self.feedback_history:\n",
    "            prompt += \"\\nPrevious feedback:\\n\"\n",
    "            prompt += \"\\n\".join(self.feedback_history[-3:])  # Include last 3 feedback items\n",
    "    \n",
    "        # Add instruction to return valid JSON\n",
    "        prompt += f\"\\n{random_variation}\\n\"\n",
    "        prompt += \"Output the data as a valid JSON object without any additional explanations or text.\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def _create_judge_criteria(self) -> str:\n",
    "        \"\"\"Create the judging criteria for evaluating the generated data.\"\"\"\n",
    "        criteria = \"\"\"\n",
    "        Evaluate the generated data based on the following criteria:\n",
    "        1. Contains all required columns (name, age, occupation)\n",
    "        2. Data types match the example data (name: string, age: integer, occupation: string)\n",
    "        3. Values are plausible and coherent (e.g., reasonable age, sensible occupation)\n",
    "        4. Absence of personally identifiable information beyond what is necessary\n",
    "        5. Similarity to the example data patterns\n",
    "    \n",
    "        Give the data a fair score even if the values are slightly different but still realistic.\n",
    "    \n",
    "        Return a score between 0 and 1, where 1 is a perfect match.\n",
    "        \"\"\"\n",
    "        return criteria\n"
   ],
   "id": "7549da99f6f4115c",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:28:01.459215Z",
     "start_time": "2024-09-23T12:27:50.274812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llms import OpenAi, IndoxApi\n",
    "\n",
    "indox = IndoxApi(api_key=indox_api_key)\n",
    "\n",
    "# Get api key from NVIDIA (https://build.nvidia.com/nvidia/nemotron-4-340b-instruct?api_key=true&)\n",
    "nemotron = OpenAi(api_key=nv_api_key, model=\"nvidia/nemotron-4-340b-instruct\",\n",
    "                  base_url=\"https://integrate.api.nvidia.com/v1\")\n",
    "nemotron_response = nemotron.chat(prompt=\"Write a limerick about the wonders of GPU computing.\",\n",
    "                                  system_prompt=\"You are a helpful assistant designed to generate synthetic data.\",\n",
    "                                  stream=True, temperature=0.2, top_p=0.7)\n",
    "print(nemotron_response)"
   ],
   "id": "d921ee5b86ad7779",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of computing, where speed is a must,\n",
      "GPU power is something we trust.\n",
      "Parallel processing, oh what a delight,\n",
      "Transforms heavy workloads, from day into night.\n",
      "With teraflops galore, it's a tech lover's lust.\n",
      "\n",
      "Nvidia, AMD, they're in the game,\n",
      "Their silicon marvels, far from tame.\n",
      "Complex calculations, in a flash,\n",
      "For AI, gaming, or a data mash.\n",
      "GPU computing, forever we'll acclaim!\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:28:12.110323Z",
     "start_time": "2024-09-23T12:28:09.904618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "indox_response = indox.chat(prompt=\"Write a limerick about the wonders of GPU computing.\",\n",
    "                            system_prompt=\"You are a helpful assistant designed to generate synthetic data.\",\n",
    "                            stream=True, temperature=0.2, top_p=0.7)\n",
    "print(indox_response)"
   ],
   "id": "ef7e474d63af810b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where the pixels collide,  \n",
      "GPUs work with incredible pride.  \n",
      "With parallel might,  \n",
      "They render the light,  \n",
      "And in data's vast ocean, they glide!\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:35:50.189574Z",
     "start_time": "2024-09-23T12:35:44.309820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up the generator\n",
    "columns = [\"name\", \"age\", \"occupation\"]\n",
    "example_data = [\n",
    "    {\"name\": \"Alice Johnson\", \"age\": 35, \"occupation\": \"Manager\"},\n",
    "    {\"name\": \"Bob Williams\", \"age\": 42, \"occupation\": \"Accountant\"}\n",
    "]\n",
    "\n",
    "generator = SyntheticDataGenerator(\n",
    "    generator_llm=indox.chat,\n",
    "    judge_llm=nemotron.chat,\n",
    "    columns=columns,\n",
    "    example_data=example_data\n",
    ")\n",
    "generated_data = generator.generate_data(num_samples=1)\n",
    "generated_data"
   ],
   "id": "fae03d940467ce3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'name': 'Catherine Smith', 'age': 29, 'occupation': 'Software Engineer'},\n",
       "  {'name': 'David Brown', 'age': 38, 'occupation': 'Graphic Designer'},\n",
       "  {'name': 'Emma Wilson', 'age': 27, 'occupation': 'Data Analyst'},\n",
       "  {'name': 'Frank Miller', 'age': 45, 'occupation': 'Project Manager'},\n",
       "  {'name': 'Grace Taylor', 'age': 31, 'occupation': 'Marketing Specialist'},\n",
       "  {'name': 'Henry Davis', 'age': 50, 'occupation': 'Sales Executive'},\n",
       "  {'name': 'Isabella Martinez', 'age': 24, 'occupation': 'Content Writer'},\n",
       "  {'name': 'James Anderson', 'age': 39, 'occupation': 'Financial Advisor'},\n",
       "  {'name': 'Sophia Thomas', 'age': 34, 'occupation': 'HR Coordinator'},\n",
       "  {'name': 'Liam Jackson', 'age': 28, 'occupation': 'Web Developer'}]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:47:08.675039Z",
     "start_time": "2024-09-23T12:46:57.633976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llms import OpenAi,HuggingFaceModel\n",
    "\n",
    "# Initialize HuggingFaceModel\n",
    "hf_model = HuggingFaceModel(api_key=hf_api_key, model=\"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "\n",
    "# Set up the generator\n",
    "columns = [\"name\", \"age\", \"occupation\"]\n",
    "example_data = [\n",
    "    {\"name\": \"Alice Johnson\", \"age\": 35, \"occupation\": \"Manager\"},\n",
    "    {\"name\": \"Bob Williams\", \"age\": 42, \"occupation\": \"Accountant\"}\n",
    "]\n",
    "\n",
    "# Get api key from NVIDIA\n",
    "nemotron = OpenAi(api_key=nv_api_key, model=\"nvidia/nemotron-4-340b-instruct\", base_url=\"https://integrate.api.nvidia.com/v1\")\n",
    "\n",
    "# Use HuggingFaceModel chat method for generation\n",
    "generator = SyntheticDataGenerator(\n",
    "    generator_llm=hf_model.chat,\n",
    "    judge_llm=nemotron.chat,\n",
    "    columns=columns,\n",
    "    example_data=example_data\n",
    ")\n",
    "\n",
    "# Generate synthetic data using the HuggingFaceModel as the generator\n",
    "generated_data = generator.generate_data(num_samples=1)\n",
    "\n",
    "# Print generated data\n",
    "print(generated_data)"
   ],
   "id": "702e1dc602cfca21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing HuggingFaceModel with model: mistralai/Mistral-Nemo-Instruct-2407\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mHuggingFaceModel initialized successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating response from Hugging Face model\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mSending request to Hugging Face API\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mReceived successful response from Hugging Face API\u001B[0m\n",
      "Failed to parse generated data as JSON: You are a synthetic data generator. Generate one realistic data based on the given examples and criteria.\n",
      "Generate synthetic data with the following columns: name, age, occupation\n",
      "The data should be similar to the following examples:\n",
      "\n",
      "{\"name\": \"Alice Johnson\", \"age\": 35, \"occupation\": \"Manager\"}\n",
      "{\"name\": \"Bob Williams\", \"age\": 42, \"occupation\": \"Accountant\"}\n",
      "\n",
      "Generate a new data point with slight variation from the examples.\n",
      "Output the data as a valid JSON object without any additional explanations or text.\n",
      "hiiii 0\n",
      "Feedback for generator: Generated data: {}\n",
      "Score: 0.0\n",
      "Reason: Low score\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating response from Hugging Face model\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mSending request to Hugging Face API\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[79], line 25\u001B[0m\n\u001B[0;32m     17\u001B[0m generator \u001B[38;5;241m=\u001B[39m SyntheticDataGenerator(\n\u001B[0;32m     18\u001B[0m     generator_llm\u001B[38;5;241m=\u001B[39mhf_model\u001B[38;5;241m.\u001B[39mchat,\n\u001B[0;32m     19\u001B[0m     judge_llm\u001B[38;5;241m=\u001B[39mnemotron\u001B[38;5;241m.\u001B[39mchat,\n\u001B[0;32m     20\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m     21\u001B[0m     example_data\u001B[38;5;241m=\u001B[39mexample_data\n\u001B[0;32m     22\u001B[0m )\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Generate synthetic data using the HuggingFaceModel as the generator\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m generated_data \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Print generated data\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(generated_data)\n",
      "Cell \u001B[1;32mIn[78], line 28\u001B[0m, in \u001B[0;36mSyntheticDataGenerator.generate_data\u001B[1;34m(self, num_samples)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_samples):\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 28\u001B[0m         generated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_single_data_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m         score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_judge_data_point(generated)\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m score \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.9\u001B[39m:  \u001B[38;5;66;03m# Perfect score\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[78], line 51\u001B[0m, in \u001B[0;36mSyntheticDataGenerator._generate_single_data_point\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     48\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_generation_prompt()\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# Call the generator LLM (Hugging Face in this case)\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m generated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerator_llm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msystem_prompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msystem_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.85\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# Print the raw output for debugging\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# print(\"Raw output from LLM:\", generated)\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;66;03m# Attempt to parse the generated data as JSON\u001B[39;00m\n",
      "File \u001B[1;32mC:\\My Files\\synthetic_data_generation\\syntethic_data\\llms\\huggingface.py:107\u001B[0m, in \u001B[0;36mHuggingFaceModel.chat\u001B[1;34m(self, prompt, system_prompt, max_tokens, temperature, top_p, frequency_penalty, presence_penalty)\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    106\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerating response from Hugging Face model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43msystem_prompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msystem_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_prompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    112\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError in chat method: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\My Files\\synthetic_data_generation\\syntethic_data\\llms\\huggingface.py:68\u001B[0m, in \u001B[0;36mHuggingFaceModel._send_request\u001B[1;34m(self, system_prompt, user_prompt, max_tokens, temperature, top_p, frequency_penalty, presence_penalty)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     67\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSending request to Hugging Face API\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 68\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://api-inference.huggingface.co/models/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m     75\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived successful response from Hugging Face API\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\requests\\api.py:115\u001B[0m, in \u001B[0;36mpost\u001B[1;34m(url, data, json, **kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    104\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request.\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \n\u001B[0;32m    106\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[0;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject4\\.venv\\Lib\\site-packages\\urllib3\\connection.py:464\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[0;32m    463\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[1;32m--> 464\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    467\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1427\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1428\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1429\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1430\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 331\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    332\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    333\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 292\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    294\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:707\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 707\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    708\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    709\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1249\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1250\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1251\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1102\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1105\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1106\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from llms import Mistral\n",
    "mistral_api = Mistral(api_key=\"your-api-key\")\n",
    "\n",
    "synthetic_data_gen = SyntheticDataGenerator(\n",
    "    generator_llm=mistral_api.chat,  # Pass the chat method\n",
    "    judge_llm=nemotron.chat,      # Pass the chat method or a custom judge function\n",
    "    columns=[\"name\", \"age\", \"occupation\"],\n",
    "    example_data=[{\"name\": \"Alice\", \"age\": 30, \"occupation\": \"Engineer\"}]\n",
    ")\n",
    "\n",
    "# Generate synthetic data\n",
    "generated_data = synthetic_data_gen.generate_data(num_samples=10)\n"
   ],
   "id": "59a4f11d3a895e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
